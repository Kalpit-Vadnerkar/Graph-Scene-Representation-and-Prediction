{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22fa957c-581f-4438-b7ab-710a26525c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da0698@unt.ad.unt.edu/anaconda3/envs/dlgpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147b0dde-1e79-4afd-93ba-033a31953598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_graph(G):\n",
    "    node_features = []\n",
    "    for node, data in G.nodes(data=True):\n",
    "        features = [\n",
    "            data['x'], data['y'],\n",
    "            data['dynamic_object_exist_probability'],\n",
    "            data['dynamic_object_position_X'], data['dynamic_object_position_Y'],\n",
    "            data['dynamic_object_velocity_X'], data['dynamic_object_velocity_Y'],\n",
    "            data['nearest_traffic_light_detection_probability']\n",
    "        ]\n",
    "        node_features.append(features)\n",
    "    \n",
    "    edge_index = []\n",
    "    for edge in G.edges():\n",
    "        edge_index.append([edge[0], edge[1]])\n",
    "        #edge_index.append([edge[1], edge[0]])  # Add reverse edge for undirected graph\n",
    "    \n",
    "    # Convert to tensors and ensure node indices are zero-based\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Adjust node indices to be zero-based\n",
    "    node_mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(G.nodes())}\n",
    "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in G.edges()], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    return x, edge_index\n",
    "\n",
    "def prepare_batch(batch_graphs, max_nodes):\n",
    "    x_seq, edge_index_seq = [], []\n",
    "    x_last, edge_index_last = [], []\n",
    "    y = []\n",
    "    batch_last = []\n",
    "    seq_lengths = []\n",
    "\n",
    "    total_nodes_last = 0\n",
    "    for batch_idx, graphs in enumerate(batch_graphs):\n",
    "        seq_x, seq_edge_index = [], []\n",
    "        for i, G in enumerate(graphs):\n",
    "            x, edge_index = preprocess_graph(G)\n",
    "\n",
    "            assert edge_index.max() < x.shape[0], f\"Edge index out of bounds for graph {i} in batch {batch_idx}\"\n",
    "\n",
    "            # Pad the graph with additional nodes if necessary\n",
    "            if x.shape[0] < max_nodes:\n",
    "                padding = torch.zeros((max_nodes - x.shape[0], x.shape[1]))\n",
    "                x = torch.cat([x, padding], dim=0)\n",
    "\n",
    "            if i < 3:\n",
    "                seq_x.append(x)\n",
    "                seq_edge_index.append(edge_index)\n",
    "            else:  # Last graph\n",
    "                x_last.append(x)\n",
    "                edge_index_last.append(edge_index + total_nodes_last)\n",
    "                y.append(x[:, 2:])  # Target features\n",
    "                batch_last.extend([batch_idx] * x.shape[0])  # Add batch index for each node\n",
    "                total_nodes_last += x.shape[0]\n",
    "\n",
    "        x_seq.append(seq_x)\n",
    "        edge_index_seq.append(seq_edge_index)\n",
    "        seq_lengths.append(len(seq_x))\n",
    "\n",
    "    # Pad x_seq\n",
    "    padded_x_seq = []\n",
    "    for batch in zip(*x_seq):\n",
    "        padded_batch = pad_sequence(batch, batch_first=True)\n",
    "        padded_x_seq.append(padded_batch)\n",
    "\n",
    "    padded_x_seq = torch.stack(padded_x_seq, dim=1)  # [batch_size, seq_len, max_nodes, features]\n",
    "\n",
    "    # Process edge_index_seq\n",
    "    max_edges = max(edge_index.shape[1] for batch in edge_index_seq for edge_index in batch)  # Maximum number of edges in the batch\n",
    "    processed_edge_index_seq = []\n",
    "    for batch in edge_index_seq:\n",
    "        batch_edge_index = []\n",
    "        for edge_index in batch:\n",
    "            # Pad edge_index if necessary\n",
    "            if edge_index.shape[1] < max_edges:\n",
    "                padding = torch.zeros((2, max_edges - edge_index.shape[1]), dtype=edge_index.dtype)\n",
    "                edge_index = torch.cat([edge_index, padding], dim=1)\n",
    "            batch_edge_index.append(edge_index)\n",
    "        processed_edge_index_seq.append(torch.stack(batch_edge_index))\n",
    "\n",
    "    edge_index_seq = torch.stack(processed_edge_index_seq)\n",
    "\n",
    "    # Concatenate all x_last and edge_index_last\n",
    "    x_last = torch.cat(x_last, dim=0)\n",
    "    edge_index_last = torch.cat(edge_index_last, dim=1)\n",
    "    y = torch.cat(y, dim=0)\n",
    "    batch_last = torch.tensor(batch_last, dtype=torch.long)\n",
    "    seq_lengths = torch.tensor(seq_lengths, dtype=torch.long)\n",
    "\n",
    "    assert edge_index_last.max() < x_last.shape[0], \"Edge index out of bounds in last graph\"\n",
    "\n",
    "    return padded_x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths\n",
    "\n",
    "def load_sequence_data(input_folder, batch_size=32):\n",
    "    all_sequences = []\n",
    "    max_nodes = 0\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        #print(f\"Processing file: {file_name}\")\n",
    "        with open(file_path, 'rb') as f:\n",
    "            sequences = pickle.load(f)\n",
    "\n",
    "            # Check if the sequences list is not empty\n",
    "            if sequences:\n",
    "                all_sequences.extend(sequences)\n",
    "\n",
    "                # Update the maximum number of nodes\n",
    "                max_nodes = max(max_nodes, max(G.number_of_nodes() for graphs in sequences for G in graphs))\n",
    "                #print(f\"Max number of nodes: {max_nodes}\")\n",
    "\n",
    "    # Shuffle the sequences\n",
    "    np.random.shuffle(all_sequences)\n",
    "\n",
    "    def batch_generator():\n",
    "        for i in range(0, len(all_sequences), batch_size):\n",
    "            batch = all_sequences[i:i+batch_size]\n",
    "\n",
    "            # Process the graphs in the batch\n",
    "            processed_batch = []\n",
    "            for graphs in batch:\n",
    "                processed_graphs = []\n",
    "                for G in graphs:\n",
    "                    processed_graphs.append(G)  # Append the graph object instead of the tuple\n",
    "                processed_batch.append(processed_graphs)\n",
    "\n",
    "            yield processed_batch\n",
    "\n",
    "    #print(f\"Max number of nodes: {max_nodes}\")\n",
    "    max_nodes = 300\n",
    "    return batch_generator(), max_nodes\n",
    "\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return F.relu(self.conv(x, edge_index))\n",
    "\n",
    "class GraphSequenceNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_gcn_layers=1, num_rnn_layers=2):\n",
    "        super(GraphSequenceNN, self).__init__()\n",
    "        \n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        self.gcn_layers.append(GCNLayer(input_dim, hidden_dim))\n",
    "        for _ in range(num_gcn_layers - 1):\n",
    "            self.gcn_layers.append(GCNLayer(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_rnn_layers, batch_first=True)\n",
    "        \n",
    "        # Final fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths = data\n",
    "        \n",
    "        batch_size, seq_len, max_nodes, _ = x_seq.size()\n",
    "        \n",
    "        gcn_out_seq = []\n",
    "        for i in range(seq_len):\n",
    "            x = x_seq[:, i]\n",
    "            edge_index = edge_index_seq[:, i]\n",
    "            \n",
    "            out = x.reshape(-1, x.size(-1))\n",
    "            for gcn_layer in self.gcn_layers:\n",
    "                out = gcn_layer(out, edge_index.reshape(2, -1))\n",
    "            \n",
    "            # Global mean pooling\n",
    "            out = global_mean_pool(out, torch.arange(batch_size).repeat_interleave(max_nodes).to(out.device))\n",
    "            gcn_out_seq.append(out)\n",
    "        \n",
    "        gcn_out_seq = torch.stack(gcn_out_seq, dim=1)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # RNN layer\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(gcn_out_seq, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        rnn_out, _ = self.rnn(packed_input)\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        \n",
    "        # Process last graph\n",
    "        out_last = x_last\n",
    "        for gcn_layer in self.gcn_layers:\n",
    "            out_last = gcn_layer(out_last, edge_index_last)\n",
    "        \n",
    "        # Combine RNN output with last graph features\n",
    "        batch_indices = torch.arange(batch_size).to(x_last.device).repeat_interleave(torch.bincount(batch_last))\n",
    "        rnn_out_last = rnn_out[batch_indices, -1]\n",
    "        combined = out_last + rnn_out_last\n",
    "        \n",
    "        # Final prediction for each node in the last graph\n",
    "        pred = self.fc(combined)\n",
    "        \n",
    "        # Apply sigmoid to probability outputs (indices 0 and 5)\n",
    "        pred[:, 0] = torch.sigmoid(pred[:, 0])  # dynamic_object_exist_probability\n",
    "        pred[:, 5] = torch.sigmoid(pred[:, 5])  # nearest_traffic_light_detection_probability\n",
    "        \n",
    "        return pred\n",
    "\n",
    "def improved_loss_function(pred, target, mask_threshold=0.85, lambda_reg=1.0, lambda_class=1.0):\n",
    "    # Extract probabilities\n",
    "    exist_prob_pred = pred[:, 0]\n",
    "    exist_prob_target = target[:, 0]\n",
    "    traffic_light_prob_pred = pred[:, 5]\n",
    "    traffic_light_prob_target = target[:, 5]\n",
    "\n",
    "    # Create mask based on existence probability\n",
    "    mask = (exist_prob_target >= mask_threshold).float()\n",
    "\n",
    "    # Classification loss (Binary Cross Entropy) for probabilities\n",
    "    bce_loss_exist = F.binary_cross_entropy_with_logits(exist_prob_pred, exist_prob_target)\n",
    "    bce_loss_traffic = F.binary_cross_entropy_with_logits(traffic_light_prob_pred, traffic_light_prob_target)\n",
    "    classification_loss = bce_loss_exist + bce_loss_traffic\n",
    "\n",
    "    # Regression loss (MSE) for positions and velocities\n",
    "    mse_loss = F.mse_loss(pred[:, 1:5], target[:, 1:5], reduction='none')\n",
    "    masked_mse_loss = (mse_loss * mask.unsqueeze(1)).mean()\n",
    "\n",
    "    # Combine losses\n",
    "    total_loss = lambda_reg * masked_mse_loss + lambda_class * classification_loss\n",
    "\n",
    "    return total_loss, masked_mse_loss, classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b5d82b-99b0-4759-8fa6-0908ef2e3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_folder = \"Training Dataset/Sequence_Dataset\"\n",
    "test_input_folder = \"Testing Dataset/Sequence_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec75304d-2a72-42a5-828f-e3542ca747ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 13.7647, Test Loss: 3.4078\n",
      "Epoch 2/100, Train Loss: 11.3322, Test Loss: 2.9482\n",
      "Epoch 3/100, Train Loss: 10.7937, Test Loss: 2.9381\n",
      "Epoch 4/100, Train Loss: 10.7636, Test Loss: 2.9310\n",
      "Epoch 5/100, Train Loss: 10.7473, Test Loss: 2.9278\n",
      "Epoch 6/100, Train Loss: 10.7326, Test Loss: 2.9250\n",
      "Epoch 7/100, Train Loss: 10.7229, Test Loss: 2.9229\n",
      "Epoch 8/100, Train Loss: 10.7162, Test Loss: 2.9218\n",
      "Epoch 9/100, Train Loss: 10.7118, Test Loss: 2.9207\n",
      "Epoch 10/100, Train Loss: 10.7081, Test Loss: 2.9200\n",
      "Epoch 11/100, Train Loss: 10.7049, Test Loss: 2.9190\n",
      "Epoch 12/100, Train Loss: 10.7019, Test Loss: 2.9185\n",
      "Epoch 13/100, Train Loss: 10.6993, Test Loss: 2.9177\n",
      "Epoch 14/100, Train Loss: 10.6965, Test Loss: 2.9169\n",
      "Epoch 15/100, Train Loss: 10.6939, Test Loss: 2.9163\n",
      "Epoch 16/100, Train Loss: 10.6915, Test Loss: 2.9157\n",
      "Epoch 17/100, Train Loss: 10.6891, Test Loss: 2.9150\n",
      "Epoch 18/100, Train Loss: 10.6870, Test Loss: 2.9146\n",
      "Epoch 19/100, Train Loss: 10.6854, Test Loss: 2.9142\n",
      "Epoch 20/100, Train Loss: 10.6840, Test Loss: 2.9140\n",
      "Epoch 21/100, Train Loss: 10.6829, Test Loss: 2.9137\n",
      "Epoch 22/100, Train Loss: 10.6821, Test Loss: 2.9135\n",
      "Epoch 23/100, Train Loss: 10.6816, Test Loss: 2.9134\n",
      "Epoch 24/100, Train Loss: 10.6811, Test Loss: 2.9134\n",
      "Epoch 25/100, Train Loss: 10.6808, Test Loss: 2.9132\n",
      "Epoch 26/100, Train Loss: 10.6805, Test Loss: 2.9133\n",
      "Epoch 27/100, Train Loss: 10.6803, Test Loss: 2.9130\n",
      "Epoch 28/100, Train Loss: 10.6801, Test Loss: 2.9130\n",
      "Epoch 29/100, Train Loss: 10.6799, Test Loss: 2.9129\n",
      "Epoch 30/100, Train Loss: 10.6797, Test Loss: 2.9129\n",
      "Epoch 31/100, Train Loss: 10.6794, Test Loss: 2.9129\n",
      "Epoch 32/100, Train Loss: 10.6794, Test Loss: 2.9127\n",
      "Epoch 33/100, Train Loss: 10.6791, Test Loss: 2.9127\n",
      "Epoch 34/100, Train Loss: 10.6790, Test Loss: 2.9126\n",
      "Epoch 35/100, Train Loss: 10.6790, Test Loss: 2.9126\n",
      "Epoch 36/100, Train Loss: 10.6790, Test Loss: 2.9126\n",
      "Epoch 37/100, Train Loss: 10.6788, Test Loss: 2.9126\n",
      "Epoch 38/100, Train Loss: 10.6786, Test Loss: 2.9125\n",
      "Epoch 39/100, Train Loss: 10.6785, Test Loss: 2.9125\n",
      "Epoch 40/100, Train Loss: 10.6784, Test Loss: 2.9124\n",
      "Epoch 41/100, Train Loss: 10.6783, Test Loss: 2.9124\n",
      "Epoch 42/100, Train Loss: 10.6782, Test Loss: 2.9125\n",
      "Epoch 43/100, Train Loss: 10.6782, Test Loss: 2.9123\n",
      "Epoch 44/100, Train Loss: 10.6781, Test Loss: 2.9123\n",
      "Epoch 45/100, Train Loss: 10.6781, Test Loss: 2.9124\n",
      "Epoch 46/100, Train Loss: 10.6780, Test Loss: 2.9122\n",
      "Epoch 47/100, Train Loss: 10.6780, Test Loss: 2.9122\n",
      "Epoch 48/100, Train Loss: 10.6779, Test Loss: 2.9122\n",
      "Epoch 49/100, Train Loss: 10.6779, Test Loss: 2.9122\n",
      "Epoch 50/100, Train Loss: 10.6780, Test Loss: 2.9122\n",
      "Epoch 51/100, Train Loss: 10.6777, Test Loss: 2.9121\n",
      "Epoch 52/100, Train Loss: 10.6778, Test Loss: 2.9121\n",
      "Epoch 53/100, Train Loss: 10.6777, Test Loss: 2.9121\n",
      "Epoch 54/100, Train Loss: 10.6777, Test Loss: 2.9121\n",
      "Epoch 55/100, Train Loss: 10.6776, Test Loss: 2.9121\n",
      "Epoch 56/100, Train Loss: 10.6776, Test Loss: 2.9121\n",
      "Epoch 57/100, Train Loss: 10.6775, Test Loss: 2.9121\n",
      "Epoch 58/100, Train Loss: 10.6775, Test Loss: 2.9121\n",
      "Epoch 59/100, Train Loss: 10.6774, Test Loss: 2.9120\n",
      "Epoch 60/100, Train Loss: 10.6774, Test Loss: 2.9120\n",
      "Epoch 61/100, Train Loss: 10.6774, Test Loss: 2.9120\n",
      "Epoch 62/100, Train Loss: 10.6774, Test Loss: 2.9120\n",
      "Epoch 63/100, Train Loss: 10.6773, Test Loss: 2.9120\n",
      "Epoch 64/100, Train Loss: 10.6773, Test Loss: 2.9121\n",
      "Epoch 65/100, Train Loss: 10.6773, Test Loss: 2.9120\n",
      "Epoch 66/100, Train Loss: 10.6773, Test Loss: 2.9120\n",
      "Epoch 67/100, Train Loss: 10.6773, Test Loss: 2.9120\n",
      "Epoch 68/100, Train Loss: 10.6773, Test Loss: 2.9120\n",
      "Epoch 69/100, Train Loss: 10.6771, Test Loss: 2.9120\n",
      "Epoch 70/100, Train Loss: 10.6773, Test Loss: 2.9120\n",
      "Epoch 71/100, Train Loss: 10.6773, Test Loss: 2.9120\n",
      "Epoch 72/100, Train Loss: 10.6773, Test Loss: 2.9121\n",
      "Epoch 73/100, Train Loss: 10.6774, Test Loss: 2.9120\n",
      "Epoch 74/100, Train Loss: 10.6771, Test Loss: 2.9119\n",
      "Epoch 75/100, Train Loss: 10.6772, Test Loss: 2.9119\n",
      "Epoch 76/100, Train Loss: 10.6771, Test Loss: 2.9119\n",
      "Epoch 77/100, Train Loss: 10.6772, Test Loss: 2.9119\n",
      "Epoch 78/100, Train Loss: 10.6770, Test Loss: 2.9119\n",
      "Epoch 79/100, Train Loss: 10.6770, Test Loss: 2.9119\n",
      "Epoch 80/100, Train Loss: 10.6769, Test Loss: 2.9119\n",
      "Epoch 81/100, Train Loss: 10.6769, Test Loss: 2.9119\n",
      "Epoch 82/100, Train Loss: 10.6769, Test Loss: 2.9119\n",
      "Epoch 83/100, Train Loss: 10.6769, Test Loss: 2.9119\n",
      "Epoch 84/100, Train Loss: 10.6769, Test Loss: 2.9119\n",
      "Epoch 85/100, Train Loss: 10.6768, Test Loss: 2.9119\n",
      "Epoch 86/100, Train Loss: 10.6769, Test Loss: 2.9118\n",
      "Epoch 87/100, Train Loss: 10.6769, Test Loss: 2.9118\n",
      "Epoch 88/100, Train Loss: 10.6768, Test Loss: 2.9119\n",
      "Epoch 89/100, Train Loss: 10.6768, Test Loss: 2.9119\n",
      "Epoch 90/100, Train Loss: 10.6769, Test Loss: 2.9118\n",
      "Epoch 91/100, Train Loss: 10.6769, Test Loss: 2.9118\n",
      "Epoch 92/100, Train Loss: 10.6768, Test Loss: 2.9118\n",
      "Epoch 93/100, Train Loss: 10.6768, Test Loss: 2.9118\n",
      "Epoch 94/100, Train Loss: 10.6768, Test Loss: 2.9118\n",
      "Epoch 95/100, Train Loss: 10.6767, Test Loss: 2.9118\n",
      "Epoch 96/100, Train Loss: 10.6767, Test Loss: 2.9118\n",
      "Epoch 97/100, Train Loss: 10.6768, Test Loss: 2.9118\n",
      "Epoch 98/100, Train Loss: 10.6768, Test Loss: 2.9118\n",
      "Epoch 99/100, Train Loss: 10.6767, Test Loss: 2.9119\n",
      "Epoch 100/100, Train Loss: 10.6767, Test Loss: 2.9118\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Predictions: tensor([[ 2.4466e-07,  3.5832e+00,  1.7789e+00,  4.0409e+00, -2.7215e-02,\n",
      "          2.9872e-07],\n",
      "        [ 5.0611e-08,  3.9758e+00,  1.9331e+00,  4.5423e+00, -5.9736e-02,\n",
      "          8.1963e-08],\n",
      "        [ 2.1807e-07,  3.9042e+00,  1.9258e+00,  3.6591e+00, -3.7309e-02,\n",
      "          4.1374e-07],\n",
      "        ...,\n",
      "        [ 9.3871e-06,  3.3413e+00,  1.6786e+00,  3.8502e+00,  1.2852e-01,\n",
      "          9.6740e-06],\n",
      "        [ 9.3871e-06,  3.3413e+00,  1.6786e+00,  3.8502e+00,  1.2852e-01,\n",
      "          9.6740e-06],\n",
      "        [ 9.3871e-06,  3.3413e+00,  1.6786e+00,  3.8502e+00,  1.2852e-01,\n",
      "          9.6740e-06]], device='cuda:0')\n",
      "Actual Values: tensor([[ 1.0000,  3.5490,  1.7886,  3.9690, -0.0062,  1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "Test Predictions: tensor([[ 4.0148e-07,  3.2990e+00,  1.5895e+00,  2.7252e+00, -2.9581e-02,\n",
      "          9.9064e-07],\n",
      "        [ 6.1891e-08,  3.5118e+00,  1.6918e+00,  2.6099e+00, -6.0209e-02,\n",
      "          2.0259e-07],\n",
      "        [ 2.2428e-07,  3.3676e+00,  1.6206e+00,  2.6778e+00, -3.9440e-02,\n",
      "          6.0640e-07],\n",
      "        ...,\n",
      "        [ 9.3842e-06,  3.3414e+00,  1.6787e+00,  3.8503e+00,  1.2853e-01,\n",
      "          9.6710e-06],\n",
      "        [ 9.3842e-06,  3.3414e+00,  1.6787e+00,  3.8503e+00,  1.2853e-01,\n",
      "          9.6710e-06],\n",
      "        [ 9.3842e-06,  3.3414e+00,  1.6787e+00,  3.8503e+00,  1.2853e-01,\n",
      "          9.6710e-06]], device='cuda:0')\n",
      "Actual Values: tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Test Predictions: tensor([[ 4.0150e-07,  3.2990e+00,  1.5895e+00,  2.7252e+00, -2.9583e-02,\n",
      "          9.9068e-07],\n",
      "        [ 6.1894e-08,  3.5118e+00,  1.6918e+00,  2.6099e+00, -6.0211e-02,\n",
      "          2.0260e-07],\n",
      "        [ 2.2429e-07,  3.3676e+00,  1.6206e+00,  2.6778e+00, -3.9443e-02,\n",
      "          6.0643e-07],\n",
      "        ...,\n",
      "        [ 9.4110e-06,  3.3406e+00,  1.6783e+00,  3.8493e+00,  1.2839e-01,\n",
      "          9.6989e-06],\n",
      "        [ 9.4110e-06,  3.3406e+00,  1.6783e+00,  3.8493e+00,  1.2839e-01,\n",
      "          9.6989e-06],\n",
      "        [ 9.4110e-06,  3.3406e+00,  1.6783e+00,  3.8493e+00,  1.2839e-01,\n",
      "          9.6989e-06]], device='cuda:0')\n",
      "Actual Values: tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 8\n",
    "hidden_dim = 128\n",
    "output_dim = 6\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 96\n",
    "lambda_reg = 0.5\n",
    "lambda_class = 0.5\n",
    "\n",
    "# Model and optimizer\n",
    "model = GraphSequenceNN(input_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize empty lists to store loss values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "\n",
    "def evaluate_model(model, test_input_folder, batch_size, max_nodes, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    test_batch_generator, _ = load_sequence_data(test_input_folder, batch_size)\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_batch_generator:\n",
    "            test_data = prepare_batch(test_batch, max_nodes)\n",
    "            x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths = [t.to(device) for t in test_data]\n",
    "            output = model((x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths))\n",
    "            loss, _, _ = improved_loss_function(output, y, lambda_reg=lambda_reg, lambda_class=lambda_class)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "# Training and evaluation loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    batch_generator, max_nodes = load_sequence_data(train_input_folder, batch_size)\n",
    "    for batch in batch_generator:\n",
    "        data = prepare_batch(batch, max_nodes)\n",
    "        x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths = [t.to(device) for t in data]\n",
    "        optimizer.zero_grad()\n",
    "        output = model((x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths))\n",
    "        loss, reg_loss, class_loss = improved_loss_function(output, y, lambda_reg=lambda_reg, lambda_class=lambda_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    # Evaluation on test data\n",
    "    total_test_loss = evaluate_model(model, test_input_folder, batch_size, max_nodes, device)\n",
    "    \n",
    "    # Append the losses of this epoch to the lists\n",
    "    train_loss_values.append(total_train_loss)\n",
    "    test_loss_values.append(total_test_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_train_loss:.4f}, Test Loss: {total_test_loss:.4f}\")\n",
    "\n",
    "# Plot the loss values\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_loss_values, 'r', label='Training loss')\n",
    "plt.plot(test_loss_values, 'b', label='Test loss')\n",
    "plt.legend()\n",
    "plt.title('Loss values')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Final evaluation on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch_generator, _ = load_sequence_data(test_input_folder, batch_size)\n",
    "    for test_batch in test_batch_generator:\n",
    "        test_data = prepare_batch(test_batch, max_nodes)\n",
    "        x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths = [t.to(device) for t in test_data]\n",
    "        predictions = model((x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths))\n",
    "        # Process predictions as needed\n",
    "        print(\"Test Predictions:\", predictions)\n",
    "        print(\"Actual Values:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8331a0c-23ad-41fc-971a-20c054a9fa76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

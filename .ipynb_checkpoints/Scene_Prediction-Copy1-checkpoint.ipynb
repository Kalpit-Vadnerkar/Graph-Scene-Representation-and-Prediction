{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22fa957c-581f-4438-b7ab-710a26525c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da0698@unt.ad.unt.edu/anaconda3/envs/dlgpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "147b0dde-1e79-4afd-93ba-033a31953598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_graph(G):\n",
    "    node_features = []\n",
    "    for node, data in G.nodes(data=True):\n",
    "        features = [\n",
    "            data['x'], data['y'],\n",
    "            data['dynamic_object_exist_probability'],\n",
    "            data['dynamic_object_position_X'], data['dynamic_object_position_Y'],\n",
    "            math.log(1 + abs(data['dynamic_object_velocity_X'])), math.log(1 + abs(data['dynamic_object_velocity_Y'])),\n",
    "            data['nearest_traffic_light_detection_probability']\n",
    "        ]\n",
    "        node_features.append(features)\n",
    "    \n",
    "    edge_index = []\n",
    "    for edge in G.edges():\n",
    "        edge_index.append([edge[0], edge[1]])\n",
    "    \n",
    "    # Convert to tensors and ensure node indices are zero-based\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Adjust node indices to be zero-based\n",
    "    node_mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(G.nodes())}\n",
    "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in G.edges()], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    return x, edge_index\n",
    "\n",
    "def prepare_batch(batch_graphs, max_nodes):\n",
    "    x_seq, edge_index_seq = [], []\n",
    "    x_last, edge_index_last = [], []\n",
    "    y = []\n",
    "    batch_last = []\n",
    "    seq_lengths = []\n",
    "\n",
    "    total_nodes_last = 0\n",
    "    for batch_idx, graphs in enumerate(batch_graphs):\n",
    "        seq_x, seq_edge_index = [], []\n",
    "        for i, G in enumerate(graphs):\n",
    "            x, edge_index = preprocess_graph(G)\n",
    "\n",
    "            assert edge_index.max() < x.shape[0], f\"Edge index out of bounds for graph {i} in batch {batch_idx}\"\n",
    "\n",
    "            # Pad the graph with additional nodes if necessary\n",
    "            if x.shape[0] < max_nodes:\n",
    "                padding = torch.zeros((max_nodes - x.shape[0], x.shape[1]))\n",
    "                x = torch.cat([x, padding], dim=0)\n",
    "\n",
    "            if i < 3:\n",
    "                seq_x.append(x)\n",
    "                seq_edge_index.append(edge_index)\n",
    "            else:  # Last graph\n",
    "                x_last.append(x)\n",
    "                edge_index_last.append(edge_index + total_nodes_last)\n",
    "                y.append(x[:, 2:])  # Target features\n",
    "                batch_last.extend([batch_idx] * x.shape[0])  # Add batch index for each node\n",
    "                total_nodes_last += x.shape[0]\n",
    "\n",
    "        x_seq.append(seq_x)\n",
    "        edge_index_seq.append(seq_edge_index)\n",
    "        seq_lengths.append(len(seq_x))\n",
    "\n",
    "    # Pad x_seq\n",
    "    padded_x_seq = []\n",
    "    for batch in zip(*x_seq):\n",
    "        padded_batch = pad_sequence(batch, batch_first=True)\n",
    "        padded_x_seq.append(padded_batch)\n",
    "\n",
    "    padded_x_seq = torch.stack(padded_x_seq, dim=1)  # [batch_size, seq_len, max_nodes, features]\n",
    "\n",
    "    # Process edge_index_seq\n",
    "    max_edges = max(edge_index.shape[1] for batch in edge_index_seq for edge_index in batch)  # Maximum number of edges in the batch\n",
    "    processed_edge_index_seq = []\n",
    "    for batch in edge_index_seq:\n",
    "        batch_edge_index = []\n",
    "        for edge_index in batch:\n",
    "            # Pad edge_index if necessary\n",
    "            if edge_index.shape[1] < max_edges:\n",
    "                padding = torch.zeros((2, max_edges - edge_index.shape[1]), dtype=edge_index.dtype)\n",
    "                edge_index = torch.cat([edge_index, padding], dim=1)\n",
    "            batch_edge_index.append(edge_index)\n",
    "        processed_edge_index_seq.append(torch.stack(batch_edge_index))\n",
    "\n",
    "    edge_index_seq = torch.stack(processed_edge_index_seq)\n",
    "\n",
    "    # Concatenate all x_last and edge_index_last\n",
    "    x_last = torch.cat(x_last, dim=0)\n",
    "    edge_index_last = torch.cat(edge_index_last, dim=1)\n",
    "    y = torch.cat(y, dim=0)\n",
    "    batch_last = torch.tensor(batch_last, dtype=torch.long)\n",
    "    seq_lengths = torch.tensor(seq_lengths, dtype=torch.long)\n",
    "\n",
    "    assert edge_index_last.max() < x_last.shape[0], \"Edge index out of bounds in last graph\"\n",
    "\n",
    "    return padded_x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths\n",
    "\n",
    "def load_sequence_data(input_folder, batch_size=32, max_nodes = 300):\n",
    "    all_sequences = []\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        #print(f\"Processing file: {file_name}\")\n",
    "        with open(file_path, 'rb') as f:\n",
    "            sequences = pickle.load(f)\n",
    "\n",
    "            # Check if the sequences list is not empty\n",
    "            if sequences:\n",
    "                all_sequences.extend(sequences)\n",
    "\n",
    "                # Update the maximum number of nodes\n",
    "                #max_nodes = max(max_nodes, max(G.number_of_nodes() for graphs in sequences for G in graphs))\n",
    "                #print(f\"Max number of nodes: {max_nodes}\")\n",
    "\n",
    "    # Shuffle the sequences\n",
    "    np.random.shuffle(all_sequences)\n",
    "\n",
    "    def batch_generator():\n",
    "        for i in range(0, len(all_sequences), batch_size):\n",
    "            batch = all_sequences[i:i+batch_size]\n",
    "\n",
    "            # Process the graphs in the batch\n",
    "            processed_batch = []\n",
    "            for graphs in batch:\n",
    "                processed_graphs = []\n",
    "                for G in graphs:\n",
    "                    processed_graphs.append(G)  # Append the graph object instead of the tuple\n",
    "                processed_batch.append(processed_graphs)\n",
    "\n",
    "            yield processed_batch\n",
    "\n",
    "    return batch_generator()\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return F.relu(self.conv(x, edge_index))\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads=1, dropout=0.6, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.gat = GATConv(in_channels, out_channels, heads=heads, dropout=dropout, concat=concat)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return F.elu(self.gat(x, edge_index))\n",
    "\n",
    "class GraphSequenceNN(nn.Module):\n",
    "    def __init__(self, input_dim, gnn_hidden_dim, rnn_hidden_dim, fc_hidden_dim, output_dim, num_gcn_layers = 1, num_gnn_layers=1, num_rnn_layers=2, heads=1):\n",
    "        super(GraphSequenceNN, self).__init__()\n",
    "        \n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        self.gcn_layers.append(GCNLayer(input_dim, gnn_hidden_dim))\n",
    "        for _ in range(num_gcn_layers - 1):\n",
    "            self.gcn_layers.append(GCNLayer(gnn_hidden_dim, gnn_hidden_dim))\n",
    "        \n",
    "        # GAT layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.gat_layers.append(GATLayer(input_dim, gnn_hidden_dim, heads=heads))\n",
    "        for _ in range(num_gat_layers - 1):\n",
    "            self.gat_layers.append(GATLayer(gnn_hidden_dim * heads, gnn_hidden_dim, heads=heads))\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.LSTM(gnn_hidden_dim * heads, rnn_hidden_dim, num_layers=num_rnn_layers, batch_first=True)\n",
    "        \n",
    "        # Final fully connected layers\n",
    "        self.fc1 = nn.Linear(gnn_hidden_dim * heads + rnn_hidden_dim, fc_hidden_dim)\n",
    "        self.fc2 = nn.Linear(fc_hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths = data\n",
    "        \n",
    "        batch_size, seq_len, max_nodes, _ = x_seq.size()\n",
    "        \n",
    "        gat_out_seq = []\n",
    "        for i in range(seq_len):\n",
    "            x = x_seq[:, i]\n",
    "            edge_index = edge_index_seq[:, i]\n",
    "            \n",
    "            out = x.reshape(-1, x.size(-1))\n",
    "            for gat_layer in self.gat_layers:\n",
    "                out = gat_layer(out, edge_index.reshape(2, -1))\n",
    "            \n",
    "            # Global mean pooling\n",
    "            out = global_mean_pool(out, torch.arange(batch_size).repeat_interleave(max_nodes).to(out.device))\n",
    "            gat_out_seq.append(out)\n",
    "        \n",
    "        gat_out_seq = torch.stack(gat_out_seq, dim=1)  # [batch_size, seq_len, gat_hidden_dim * heads]\n",
    "        \n",
    "        # RNN layer\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(gat_out_seq, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        rnn_out, _ = self.rnn(packed_input)\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        \n",
    "        # Process last graph\n",
    "        out_last = x_last\n",
    "        for gat_layer in self.gat_layers:\n",
    "            out_last = gat_layer(out_last, edge_index_last)\n",
    "        \n",
    "        # Combine RNN output with last graph features\n",
    "        batch_indices = torch.arange(batch_size).to(x_last.device).repeat_interleave(torch.bincount(batch_last))\n",
    "        rnn_out_last = rnn_out[batch_indices, -1]\n",
    "        combined = torch.cat([out_last, rnn_out_last], dim=-1)\n",
    "        \n",
    "        # Final prediction for each node in the last graph\n",
    "        hidden = F.relu(self.fc1(combined))\n",
    "        pred = self.fc2(hidden)\n",
    "        \n",
    "        # Apply sigmoid to probability outputs (indices 0 and 5)\n",
    "        pred[:, 0] = torch.sigmoid(pred[:, 0])  # dynamic_object_exist_probability\n",
    "        pred[:, 5] = torch.sigmoid(pred[:, 5])  # nearest_traffic_light_detection_probability\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "def improved_loss_function(pred, target, mask_threshold=0.5, lambda_reg=1.0, lambda_class=1.0, pos_weight=1.0):\n",
    "    # Extract probabilities\n",
    "    exist_prob_pred = pred[:, 0]\n",
    "    exist_prob_target = target[:, 0]\n",
    "    traffic_light_prob_pred = pred[:, 5]\n",
    "    traffic_light_prob_target = target[:, 5]\n",
    "\n",
    "    # Create mask based on existence probability\n",
    "    mask = (exist_prob_target >= mask_threshold).float()\n",
    "    #mask = (exist_prob_pred >= mask_threshold).float()\n",
    "\n",
    "    # Classification loss (Binary Cross Entropy) for probabilities\n",
    "    \n",
    "    # Weighted Binary Cross Entropy Loss\n",
    "    weight_exist = torch.where(exist_prob_target == 1, pos_weight, 0.001)  # Higher weight when target is 1\n",
    "    weight_traffic = torch.where(traffic_light_prob_target == 1, pos_weight, 0.01)\n",
    "\n",
    "    bce_loss_exist = F.binary_cross_entropy_with_logits(exist_prob_pred, exist_prob_target, weight=weight_exist)\n",
    "    bce_loss_traffic = F.binary_cross_entropy_with_logits(traffic_light_prob_pred, traffic_light_prob_target, weight=weight_traffic)\n",
    "    classification_loss = bce_loss_exist + bce_loss_traffic\n",
    "\n",
    "    # Regression loss (MSE) for positions and velocities\n",
    "    mse_loss = F.mse_loss(pred[:, 1:5], target[:, 1:5], reduction='none')\n",
    "    masked_mse_loss = (mse_loss * mask.unsqueeze(1)).mean()\n",
    "\n",
    "    # Combine losses\n",
    "    total_loss = lambda_reg * masked_mse_loss + lambda_class * classification_loss\n",
    "\n",
    "    return total_loss, masked_mse_loss, classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b5d82b-99b0-4759-8fa6-0908ef2e3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1 (node distance = 5) => fewer nodes \n",
    "# Dataset 2 (node distance = 1)\n",
    "train_input_folder = \"Training Dataset1/Sequence_Dataset\"\n",
    "test_input_folder = \"Testing Dataset1/Sequence_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75304d-2a72-42a5-828f-e3542ca747ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.7475, Test Loss: 0.2110\n",
      "Epoch 2/50, Train Loss: 0.7299, Test Loss: 0.2152\n",
      "Epoch 3/50, Train Loss: 0.6908, Test Loss: 0.1882\n",
      "Epoch 4/50, Train Loss: 0.6895, Test Loss: 0.1784\n",
      "Epoch 5/50, Train Loss: 0.6618, Test Loss: 0.1818\n",
      "Epoch 6/50, Train Loss: 0.6548, Test Loss: 0.1861\n",
      "Epoch 7/50, Train Loss: 0.6256, Test Loss: 0.1857\n",
      "Epoch 8/50, Train Loss: 0.6432, Test Loss: 0.1824\n",
      "Epoch 9/50, Train Loss: 0.6394, Test Loss: 0.1814\n",
      "Epoch 10/50, Train Loss: 0.6379, Test Loss: 0.1731\n",
      "Epoch 11/50, Train Loss: 0.6098, Test Loss: 0.1832\n",
      "Epoch 12/50, Train Loss: 0.6029, Test Loss: 0.1595\n",
      "Epoch 13/50, Train Loss: 0.6152, Test Loss: 0.1721\n",
      "Epoch 14/50, Train Loss: 0.6063, Test Loss: 0.1617\n",
      "Epoch 15/50, Train Loss: 0.5993, Test Loss: 0.1692\n",
      "Epoch 16/50, Train Loss: 0.5978, Test Loss: 0.1889\n",
      "Epoch 17/50, Train Loss: 0.6027, Test Loss: 0.1650\n",
      "Epoch 18/50, Train Loss: 0.6132, Test Loss: 0.1645\n",
      "Epoch 19/50, Train Loss: 0.6020, Test Loss: 0.1855\n",
      "Epoch 20/50, Train Loss: 0.5929, Test Loss: 0.1723\n",
      "Epoch 21/50, Train Loss: 0.5868, Test Loss: 0.1757\n",
      "Epoch 22/50, Train Loss: 0.5865, Test Loss: 0.1682\n",
      "Epoch 23/50, Train Loss: 0.5846, Test Loss: 0.1692\n",
      "Epoch 24/50, Train Loss: 0.5964, Test Loss: 0.1699\n",
      "Epoch 25/50, Train Loss: 0.5810, Test Loss: 0.1831\n",
      "Epoch 26/50, Train Loss: 0.5732, Test Loss: 0.1781\n",
      "Epoch 27/50, Train Loss: 0.5937, Test Loss: 0.1892\n",
      "Epoch 28/50, Train Loss: 0.5922, Test Loss: 0.1648\n",
      "Epoch 29/50, Train Loss: 0.5891, Test Loss: 0.1589\n",
      "Epoch 30/50, Train Loss: 0.5975, Test Loss: 0.1573\n",
      "Epoch 31/50, Train Loss: 0.5879, Test Loss: 0.1829\n",
      "Epoch 32/50, Train Loss: 0.5862, Test Loss: 0.1804\n",
      "Epoch 33/50, Train Loss: 0.5827, Test Loss: 0.1778\n",
      "Epoch 34/50, Train Loss: 0.5813, Test Loss: 0.1697\n",
      "Epoch 35/50, Train Loss: 0.6005, Test Loss: 0.1722\n",
      "Epoch 36/50, Train Loss: 0.5804, Test Loss: 0.1759\n",
      "Epoch 37/50, Train Loss: 0.5752, Test Loss: 0.1597\n",
      "Epoch 38/50, Train Loss: 0.5681, Test Loss: 0.1687\n",
      "Epoch 39/50, Train Loss: 0.5712, Test Loss: 0.1570\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "max_nodes = 300\n",
    "input_dim = 8\n",
    "gnn_hidden_dim = 32\n",
    "rnn_hidden_dim = 64\n",
    "fc_hidden_dim = 48\n",
    "output_dim = 6\n",
    "heads = 1\n",
    "num_gcn_layers = 1\n",
    "num_gat_layers = 0 \n",
    "num_rnn_layers = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "lambda_reg = 1\n",
    "lambda_class = 1\n",
    "# Model and optimizer\n",
    "model = GraphSequenceNN(input_dim, \n",
    "                        gnn_hidden_dim, \n",
    "                        rnn_hidden_dim, \n",
    "                        fc_hidden_dim, \n",
    "                        output_dim,\n",
    "                        num_gcn_layers,\n",
    "                        num_gat_layers, \n",
    "                        num_rnn_layers, heads)\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "# Initialize scheduler\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "# Initialize empty lists to store loss values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "def evaluate_model(model, test_input_folder, batch_size, max_nodes, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    test_batch_generator = load_sequence_data(test_input_folder, batch_size)\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_batch_generator:\n",
    "            test_data = prepare_batch(test_batch, max_nodes)\n",
    "            x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths = [t.to(device) for t in test_data]\n",
    "            output = model((x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths))\n",
    "            loss, _, _ = improved_loss_function(output, y, lambda_reg=lambda_reg, lambda_class=lambda_class)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "# Training and evaluation loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    batch_generator = load_sequence_data(train_input_folder, batch_size)\n",
    "    for batch in batch_generator:\n",
    "        data = prepare_batch(batch, max_nodes)\n",
    "        x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths = [t.to(device) for t in data]\n",
    "        optimizer.zero_grad()\n",
    "        output = model((x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths))\n",
    "        loss, reg_loss, class_loss = improved_loss_function(output, y, lambda_reg=lambda_reg, lambda_class=lambda_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Evaluation on test data\n",
    "    total_test_loss = evaluate_model(model, test_input_folder, batch_size, max_nodes, device)\n",
    "\n",
    "    # Append the losses of this epoch to the lists\n",
    "    train_loss_values.append(total_train_loss)\n",
    "    test_loss_values.append(total_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_train_loss:.4f}, Test Loss: {total_test_loss:.4f}\")\n",
    "    scheduler.step()\n",
    "# Plot the loss values\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_loss_values, 'r', label='Training loss')\n",
    "plt.plot(test_loss_values, 'b', label='Test loss')\n",
    "plt.legend()\n",
    "plt.title('Loss values')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f255d-d583-41b7-a476-961d72af4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_and_actual(G, predictions, actual, filename):\n",
    "    # Create a combined dictionary with x and y coordinates\n",
    "    pos = {node: (data['x'], data['y']) for node, data in G.nodes(data=True)}\n",
    "\n",
    "    # Separate nodes by type\n",
    "    map_nodes = [node for node, data in G.nodes(data=True) if data['type'] == 'map_node']\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Draw map nodes\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=map_nodes, node_color='lightgray', node_size=2, ax=ax)\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='lightgray', ax=ax)\n",
    "\n",
    "    # Plot actual dynamic objects\n",
    "    actual_dynamic_objects = [node for node, data in G.nodes(data=True) if actual[node][0] == 1]\n",
    "    print(f\"Number of actual dynamic objects: {len(actual_dynamic_objects)}\")\n",
    "    #Dyna_pos = {node: (node[1], node[2]) for node in actual_dynamic_objects}\n",
    "    #nx.draw_networkx_nodes(G, Dyna_pos, nodelist=actual_dynamic_objects, node_color='blue', node_size=50, alpha=0.7, label='Actual Dynamic Objects', ax=ax)\n",
    "\n",
    "    # Plot predicted dynamic objects\n",
    "    predicted_dynamic_objects = [node for node, data in G.nodes(data=True) if predictions[node][0] >= 0.85]\n",
    "    print(f\"Number of predicted dynamic objects: {len(predicted_dynamic_objects)}\")\n",
    "    #nx.draw_networkx_nodes(G, pos, nodelist=predicted_dynamic_objects, node_color='red', node_size=50, alpha=0.7, label='Predicted Dynamic Objects', ax=ax)\n",
    "\n",
    "    # Plot actual traffic lights\n",
    "    actual_traffic_lights = [node for node, data in G.nodes(data=True) if actual[node][5] == 1]\n",
    "    print(f\"Number of actual traffic lights: {len(actual_traffic_lights)}\")\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=actual_traffic_lights, node_color='green', node_size=50, alpha=0.7, label='Actual Traffic Lights', ax=ax)\n",
    "\n",
    "    # Plot predicted traffic lights\n",
    "    predicted_traffic_lights = [node for node, data in G.nodes(data=True) if predictions[node][5] >= 0.85]\n",
    "    print(f\"Number of predicted traffic lights: {len(predicted_traffic_lights)}\")\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=predicted_traffic_lights, node_color='orange', node_size=50, alpha=0.7, label='Predicted Traffic Lights', ax=ax)\n",
    "\n",
    "    # Set axis limits to fit the data\n",
    "    x_values, y_values = zip(*pos.values())\n",
    "    padding = 0\n",
    "    ax.set_xlim(min(x_values) - padding, max(x_values) + padding)\n",
    "    ax.set_ylim(min(y_values) - padding, max(y_values) + padding)\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a90ab-e06b-4a08-903c-6b7cd8fb8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch_generator = load_sequence_data(test_input_folder, batch_size)\n",
    "    for i, test_batch in enumerate(test_batch_generator):\n",
    "        test_data = prepare_batch(test_batch, max_nodes)\n",
    "        x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths = [t.to(device) for t in test_data]\n",
    "        predictions = model((x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths))\n",
    "        \n",
    "        # Move tensors to CPU before converting to numpy\n",
    "        predictions = predictions.cpu()\n",
    "        y = y.cpu()\n",
    "        batch_last = batch_last.cpu()\n",
    "        \n",
    "        # Convert predictions and actual values to numpy for easier handling\n",
    "        predictions = predictions.numpy()\n",
    "        y = y.numpy()\n",
    "        batch_last = batch_last.numpy()\n",
    "        \n",
    "        # For each graph in the batch\n",
    "        for j, G in enumerate(test_batch[-1]):  # We're interested in the last graph of each sequence\n",
    "            # Get the number of nodes in this graph\n",
    "            num_nodes = G.number_of_nodes()\n",
    "            \n",
    "            # Extract predictions and actual values for this graph\n",
    "            graph_predictions = predictions[batch_last == j]\n",
    "            graph_actual = y[batch_last == j]\n",
    "            \n",
    "            # Create dictionaries mapping node IDs to their predictions and actual values\n",
    "            pred_dict = {node: graph_predictions[i] for i, node in enumerate(G.nodes())}\n",
    "            actual_dict = {node: graph_actual[i] for i, node in enumerate(G.nodes())}\n",
    "            \n",
    "            # Plot and save the graph\n",
    "            plot_predictions_and_actual(G, pred_dict, actual_dict, f'plots/graph_prediction_{i}_{j}.png')\n",
    "\n",
    "        print(f\"Processed batch {i}, graphs visualized and saved.\")\n",
    "\n",
    "print(\"Evaluation and visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb038b13-8dec-4b03-beb4-1df4f833954e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

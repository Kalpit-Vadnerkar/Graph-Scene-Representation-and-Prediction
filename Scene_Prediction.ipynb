{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22fa957c-581f-4438-b7ab-710a26525c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da0698@unt.ad.unt.edu/anaconda3/envs/dlgpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147b0dde-1e79-4afd-93ba-033a31953598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_graph(G):\n",
    "    node_features = []\n",
    "    for node, data in G.nodes(data=True):\n",
    "        features = [\n",
    "            data['x'], data['y'],\n",
    "            data['dynamic_object_exist_probability'],\n",
    "            data['dynamic_object_position_X'], data['dynamic_object_position_Y'],\n",
    "            data['dynamic_object_velocity_X'], data['dynamic_object_velocity_Y'],\n",
    "            data['nearest_traffic_light_detection_probability']\n",
    "        ]\n",
    "        node_features.append(features)\n",
    "    \n",
    "    edge_index = []\n",
    "    for edge in G.edges():\n",
    "        edge_index.append([edge[0], edge[1]])\n",
    "        #edge_index.append([edge[1], edge[0]])  # Add reverse edge for undirected graph\n",
    "    \n",
    "    # Convert to tensors and ensure node indices are zero-based\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    # Adjust node indices to be zero-based\n",
    "    node_mapping = {old_idx: new_idx for new_idx, old_idx in enumerate(G.nodes())}\n",
    "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in G.edges()], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    return x, edge_index\n",
    "\n",
    "def prepare_batch(batch_graphs, max_nodes):\n",
    "    x_seq, edge_index_seq = [], []\n",
    "    x_last, edge_index_last = [], []\n",
    "    y = []\n",
    "    batch_last = []\n",
    "    seq_lengths = []\n",
    "\n",
    "    total_nodes_last = 0\n",
    "    for batch_idx, graphs in enumerate(batch_graphs):\n",
    "        seq_x, seq_edge_index = [], []\n",
    "        for i, G in enumerate(graphs):\n",
    "            x, edge_index = preprocess_graph(G)\n",
    "\n",
    "            assert edge_index.max() < x.shape[0], f\"Edge index out of bounds for graph {i} in batch {batch_idx}\"\n",
    "\n",
    "            # Pad the graph with additional nodes if necessary\n",
    "            if x.shape[0] < max_nodes:\n",
    "                padding = torch.zeros((max_nodes - x.shape[0], x.shape[1]))\n",
    "                x = torch.cat([x, padding], dim=0)\n",
    "\n",
    "            if i < 3:\n",
    "                seq_x.append(x)\n",
    "                seq_edge_index.append(edge_index)\n",
    "            else:  # Last graph\n",
    "                x_last.append(x)\n",
    "                edge_index_last.append(edge_index + total_nodes_last)\n",
    "                y.append(x[:, 2:])  # Target features\n",
    "                batch_last.extend([batch_idx] * x.shape[0])  # Add batch index for each node\n",
    "                total_nodes_last += x.shape[0]\n",
    "\n",
    "        x_seq.append(seq_x)\n",
    "        edge_index_seq.append(seq_edge_index)\n",
    "        seq_lengths.append(len(seq_x))\n",
    "\n",
    "    # Pad x_seq\n",
    "    padded_x_seq = []\n",
    "    for batch in zip(*x_seq):\n",
    "        padded_batch = pad_sequence(batch, batch_first=True)\n",
    "        padded_x_seq.append(padded_batch)\n",
    "\n",
    "    padded_x_seq = torch.stack(padded_x_seq, dim=1)  # [batch_size, seq_len, max_nodes, features]\n",
    "\n",
    "    # Process edge_index_seq\n",
    "    max_edges = max(edge_index.shape[1] for batch in edge_index_seq for edge_index in batch)  # Maximum number of edges in the batch\n",
    "    processed_edge_index_seq = []\n",
    "    for batch in edge_index_seq:\n",
    "        batch_edge_index = []\n",
    "        for edge_index in batch:\n",
    "            # Pad edge_index if necessary\n",
    "            if edge_index.shape[1] < max_edges:\n",
    "                padding = torch.zeros((2, max_edges - edge_index.shape[1]), dtype=edge_index.dtype)\n",
    "                edge_index = torch.cat([edge_index, padding], dim=1)\n",
    "            batch_edge_index.append(edge_index)\n",
    "        processed_edge_index_seq.append(torch.stack(batch_edge_index))\n",
    "\n",
    "    edge_index_seq = torch.stack(processed_edge_index_seq)\n",
    "\n",
    "    # Concatenate all x_last and edge_index_last\n",
    "    x_last = torch.cat(x_last, dim=0)\n",
    "    edge_index_last = torch.cat(edge_index_last, dim=1)\n",
    "    y = torch.cat(y, dim=0)\n",
    "    batch_last = torch.tensor(batch_last, dtype=torch.long)\n",
    "    seq_lengths = torch.tensor(seq_lengths, dtype=torch.long)\n",
    "\n",
    "    assert edge_index_last.max() < x_last.shape[0], \"Edge index out of bounds in last graph\"\n",
    "\n",
    "    return padded_x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths\n",
    "\n",
    "def load_sequence_data(input_folder, batch_size=32):\n",
    "    all_sequences = []\n",
    "    max_nodes = 0\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        #print(f\"Processing file: {file_name}\")\n",
    "        with open(file_path, 'rb') as f:\n",
    "            sequences = pickle.load(f)\n",
    "\n",
    "            # Check if the sequences list is not empty\n",
    "            if sequences:\n",
    "                all_sequences.extend(sequences)\n",
    "\n",
    "                # Update the maximum number of nodes\n",
    "                max_nodes = max(max_nodes, max(G.number_of_nodes() for graphs in sequences for G in graphs))\n",
    "                #print(f\"Max number of nodes: {max_nodes}\")\n",
    "\n",
    "    # Shuffle the sequences\n",
    "    np.random.shuffle(all_sequences)\n",
    "\n",
    "    def batch_generator():\n",
    "        for i in range(0, len(all_sequences), batch_size):\n",
    "            batch = all_sequences[i:i+batch_size]\n",
    "\n",
    "            # Process the graphs in the batch\n",
    "            processed_batch = []\n",
    "            for graphs in batch:\n",
    "                processed_graphs = []\n",
    "                for G in graphs:\n",
    "                    processed_graphs.append(G)  # Append the graph object instead of the tuple\n",
    "                processed_batch.append(processed_graphs)\n",
    "\n",
    "            yield processed_batch\n",
    "\n",
    "    #print(f\"Max number of nodes: {max_nodes}\")\n",
    "    max_nodes = 300 # Dataset 1\n",
    "    #max_nodes = 600 # Dataset 2\n",
    "    return batch_generator(), max_nodes\n",
    "\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return F.relu(self.conv(x, edge_index))\n",
    "\n",
    "class GraphSequenceNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_gcn_layers=1, num_rnn_layers=2):\n",
    "        super(GraphSequenceNN, self).__init__()\n",
    "        \n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        self.gcn_layers.append(GCNLayer(input_dim, hidden_dim))\n",
    "        for _ in range(num_gcn_layers - 1):\n",
    "            self.gcn_layers.append(GCNLayer(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_rnn_layers, batch_first=True)\n",
    "        \n",
    "        # Final fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths = data\n",
    "        \n",
    "        batch_size, seq_len, max_nodes, _ = x_seq.size()\n",
    "        \n",
    "        gcn_out_seq = []\n",
    "        for i in range(seq_len):\n",
    "            x = x_seq[:, i]\n",
    "            edge_index = edge_index_seq[:, i]\n",
    "            \n",
    "            out = x.reshape(-1, x.size(-1))\n",
    "            for gcn_layer in self.gcn_layers:\n",
    "                out = gcn_layer(out, edge_index.reshape(2, -1))\n",
    "            \n",
    "            # Global mean pooling\n",
    "            out = global_mean_pool(out, torch.arange(batch_size).repeat_interleave(max_nodes).to(out.device))\n",
    "            gcn_out_seq.append(out)\n",
    "        \n",
    "        gcn_out_seq = torch.stack(gcn_out_seq, dim=1)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # RNN layer\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(gcn_out_seq, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        rnn_out, _ = self.rnn(packed_input)\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        \n",
    "        # Process last graph\n",
    "        out_last = x_last\n",
    "        for gcn_layer in self.gcn_layers:\n",
    "            out_last = gcn_layer(out_last, edge_index_last)\n",
    "        \n",
    "        # Combine RNN output with last graph features\n",
    "        batch_indices = torch.arange(batch_size).to(x_last.device).repeat_interleave(torch.bincount(batch_last))\n",
    "        rnn_out_last = rnn_out[batch_indices, -1]\n",
    "        combined = out_last + rnn_out_last\n",
    "        \n",
    "        # Final prediction for each node in the last graph\n",
    "        pred = self.fc(combined)\n",
    "        \n",
    "        # Apply sigmoid to probability outputs (indices 0 and 5)\n",
    "        pred[:, 0] = torch.sigmoid(pred[:, 0])  # dynamic_object_exist_probability\n",
    "        pred[:, 5] = torch.sigmoid(pred[:, 5])  # nearest_traffic_light_detection_probability\n",
    "        \n",
    "        return pred\n",
    "\n",
    "def improved_loss_function(pred, target, mask_threshold=0.85, lambda_reg=1.0, lambda_class=1.0):\n",
    "    # Extract probabilities\n",
    "    exist_prob_pred = pred[:, 0]\n",
    "    exist_prob_target = target[:, 0]\n",
    "    traffic_light_prob_pred = pred[:, 5]\n",
    "    traffic_light_prob_target = target[:, 5]\n",
    "\n",
    "    # Create mask based on existence probability\n",
    "    mask = (exist_prob_target >= mask_threshold).float()\n",
    "\n",
    "    # Classification loss (Binary Cross Entropy) for probabilities\n",
    "    bce_loss_exist = F.binary_cross_entropy_with_logits(exist_prob_pred, exist_prob_target)\n",
    "    bce_loss_traffic = F.binary_cross_entropy_with_logits(traffic_light_prob_pred, traffic_light_prob_target)\n",
    "    classification_loss = bce_loss_exist + bce_loss_traffic\n",
    "\n",
    "    # Regression loss (MSE) for positions and velocities\n",
    "    mse_loss = F.mse_loss(pred[:, 1:5], target[:, 1:5], reduction='none')\n",
    "    masked_mse_loss = (mse_loss * mask.unsqueeze(1)).mean()\n",
    "\n",
    "    # Combine losses\n",
    "    total_loss = lambda_reg * masked_mse_loss + lambda_class * classification_loss\n",
    "\n",
    "    return total_loss, masked_mse_loss, classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b5d82b-99b0-4759-8fa6-0908ef2e3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1 (node distance = 5) => fewer nodes \n",
    "# Dataset 2 (node distance = 1)\n",
    "train_input_folder = \"Training Dataset1/Sequence_Dataset\"\n",
    "test_input_folder = \"Testing Dataset1/Sequence_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec75304d-2a72-42a5-828f-e3542ca747ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 8.3868, Test Loss: 2.6700\n",
      "Epoch 2/100, Train Loss: 7.3484, Test Loss: 2.2087\n",
      "Epoch 3/100, Train Loss: 6.4789, Test Loss: 2.1302\n",
      "Epoch 4/100, Train Loss: 6.3815, Test Loss: 2.1234\n",
      "Epoch 5/100, Train Loss: 6.3678, Test Loss: 2.1141\n",
      "Epoch 6/100, Train Loss: 6.3455, Test Loss: 2.1076\n",
      "Epoch 7/100, Train Loss: 6.3414, Test Loss: 2.1041\n",
      "Epoch 8/100, Train Loss: 6.3192, Test Loss: 2.0977\n",
      "Epoch 9/100, Train Loss: 6.3123, Test Loss: 2.0984\n",
      "Epoch 10/100, Train Loss: 6.2982, Test Loss: 2.0961\n",
      "Epoch 11/100, Train Loss: 6.2885, Test Loss: 2.0909\n",
      "Epoch 12/100, Train Loss: 6.2778, Test Loss: 2.0918\n",
      "Epoch 13/100, Train Loss: 6.2671, Test Loss: 2.0869\n",
      "Epoch 14/100, Train Loss: 6.2623, Test Loss: 2.0856\n",
      "Epoch 15/100, Train Loss: 6.2571, Test Loss: 2.0850\n",
      "Epoch 16/100, Train Loss: 6.2543, Test Loss: 2.0847\n",
      "Epoch 17/100, Train Loss: 6.2526, Test Loss: 2.0834\n",
      "Epoch 18/100, Train Loss: 6.2536, Test Loss: 2.0839\n",
      "Epoch 19/100, Train Loss: 6.2515, Test Loss: 2.0849\n",
      "Epoch 20/100, Train Loss: 6.2522, Test Loss: 2.0828\n",
      "Epoch 21/100, Train Loss: 6.2511, Test Loss: 2.0841\n",
      "Epoch 22/100, Train Loss: 6.2508, Test Loss: 2.0861\n",
      "Epoch 23/100, Train Loss: 6.2526, Test Loss: 2.0851\n",
      "Epoch 24/100, Train Loss: 6.2503, Test Loss: 2.0841\n",
      "Epoch 25/100, Train Loss: 6.2498, Test Loss: 2.0826\n",
      "Epoch 26/100, Train Loss: 6.2499, Test Loss: 2.0828\n",
      "Epoch 27/100, Train Loss: 6.2508, Test Loss: 2.0826\n",
      "Epoch 28/100, Train Loss: 6.2496, Test Loss: 2.0832\n",
      "Epoch 29/100, Train Loss: 6.2495, Test Loss: 2.0825\n",
      "Epoch 30/100, Train Loss: 6.2494, Test Loss: 2.0824\n",
      "Epoch 31/100, Train Loss: 6.2499, Test Loss: 2.0829\n",
      "Epoch 32/100, Train Loss: 6.2500, Test Loss: 2.0826\n",
      "Epoch 33/100, Train Loss: 6.2513, Test Loss: 2.0824\n",
      "Epoch 34/100, Train Loss: 6.2489, Test Loss: 2.0828\n",
      "Epoch 35/100, Train Loss: 6.2492, Test Loss: 2.0839\n",
      "Epoch 36/100, Train Loss: 6.2498, Test Loss: 2.0825\n",
      "Epoch 37/100, Train Loss: 6.2492, Test Loss: 2.0830\n",
      "Epoch 38/100, Train Loss: 6.2504, Test Loss: 2.0828\n",
      "Epoch 39/100, Train Loss: 6.2491, Test Loss: 2.0828\n",
      "Epoch 40/100, Train Loss: 6.2484, Test Loss: 2.0831\n",
      "Epoch 41/100, Train Loss: 6.2505, Test Loss: 2.0822\n",
      "Epoch 42/100, Train Loss: 6.2480, Test Loss: 2.0828\n",
      "Epoch 43/100, Train Loss: 6.2486, Test Loss: 2.0843\n",
      "Epoch 44/100, Train Loss: 6.2484, Test Loss: 2.0832\n",
      "Epoch 45/100, Train Loss: 6.2480, Test Loss: 2.0830\n",
      "Epoch 46/100, Train Loss: 6.2478, Test Loss: 2.0832\n",
      "Epoch 47/100, Train Loss: 6.2487, Test Loss: 2.0825\n",
      "Epoch 48/100, Train Loss: 6.2474, Test Loss: 2.0831\n",
      "Epoch 49/100, Train Loss: 6.2480, Test Loss: 2.0818\n",
      "Epoch 50/100, Train Loss: 6.2479, Test Loss: 2.0828\n",
      "Epoch 51/100, Train Loss: 6.2477, Test Loss: 2.0828\n",
      "Epoch 52/100, Train Loss: 6.2484, Test Loss: 2.0824\n",
      "Epoch 53/100, Train Loss: 6.2479, Test Loss: 2.0827\n",
      "Epoch 54/100, Train Loss: 6.2475, Test Loss: 2.0833\n",
      "Epoch 55/100, Train Loss: 6.2476, Test Loss: 2.0820\n",
      "Epoch 56/100, Train Loss: 6.2474, Test Loss: 2.0822\n",
      "Epoch 57/100, Train Loss: 6.2468, Test Loss: 2.0834\n",
      "Epoch 58/100, Train Loss: 6.2473, Test Loss: 2.0819\n",
      "Epoch 59/100, Train Loss: 6.2482, Test Loss: 2.0818\n",
      "Epoch 60/100, Train Loss: 6.2489, Test Loss: 2.0824\n",
      "Epoch 61/100, Train Loss: 6.2463, Test Loss: 2.0814\n",
      "Epoch 62/100, Train Loss: 6.2460, Test Loss: 2.0824\n",
      "Epoch 63/100, Train Loss: 6.2464, Test Loss: 2.0816\n",
      "Epoch 64/100, Train Loss: 6.2468, Test Loss: 2.0816\n",
      "Epoch 65/100, Train Loss: 6.2471, Test Loss: 2.0820\n",
      "Epoch 66/100, Train Loss: 6.2469, Test Loss: 2.0837\n",
      "Epoch 67/100, Train Loss: 6.2462, Test Loss: 2.0816\n",
      "Epoch 68/100, Train Loss: 6.2460, Test Loss: 2.0816\n",
      "Epoch 69/100, Train Loss: 6.2459, Test Loss: 2.0817\n",
      "Epoch 70/100, Train Loss: 6.2455, Test Loss: 2.0821\n",
      "Epoch 71/100, Train Loss: 6.2461, Test Loss: 2.0821\n",
      "Epoch 72/100, Train Loss: 6.2459, Test Loss: 2.0812\n",
      "Epoch 73/100, Train Loss: 6.2453, Test Loss: 2.0812\n",
      "Epoch 74/100, Train Loss: 6.2453, Test Loss: 2.0814\n",
      "Epoch 75/100, Train Loss: 6.2453, Test Loss: 2.0815\n",
      "Epoch 76/100, Train Loss: 6.2460, Test Loss: 2.0811\n",
      "Epoch 77/100, Train Loss: 6.2460, Test Loss: 2.0815\n",
      "Epoch 78/100, Train Loss: 6.2453, Test Loss: 2.0832\n",
      "Epoch 79/100, Train Loss: 6.2452, Test Loss: 2.0816\n",
      "Epoch 80/100, Train Loss: 6.2448, Test Loss: 2.0812\n",
      "Epoch 81/100, Train Loss: 6.2449, Test Loss: 2.0810\n",
      "Epoch 82/100, Train Loss: 6.2454, Test Loss: 2.0811\n",
      "Epoch 83/100, Train Loss: 6.2451, Test Loss: 2.0810\n",
      "Epoch 84/100, Train Loss: 6.2449, Test Loss: 2.0809\n",
      "Epoch 85/100, Train Loss: 6.2443, Test Loss: 2.0813\n",
      "Epoch 86/100, Train Loss: 6.2445, Test Loss: 2.0816\n",
      "Epoch 87/100, Train Loss: 6.2445, Test Loss: 2.0811\n",
      "Epoch 88/100, Train Loss: 6.2444, Test Loss: 2.0816\n",
      "Epoch 89/100, Train Loss: 6.2441, Test Loss: 2.0816\n",
      "Epoch 90/100, Train Loss: 6.2440, Test Loss: 2.0808\n",
      "Epoch 91/100, Train Loss: 6.2440, Test Loss: 2.0817\n",
      "Epoch 92/100, Train Loss: 6.2442, Test Loss: 2.0808\n",
      "Epoch 93/100, Train Loss: 6.2436, Test Loss: 2.0808\n",
      "Epoch 94/100, Train Loss: 6.2443, Test Loss: 2.0808\n",
      "Epoch 95/100, Train Loss: 6.2436, Test Loss: 2.0808\n",
      "Epoch 96/100, Train Loss: 6.2436, Test Loss: 2.0811\n",
      "Epoch 97/100, Train Loss: 6.2435, Test Loss: 2.0808\n",
      "Epoch 98/100, Train Loss: 6.2439, Test Loss: 2.0812\n",
      "Epoch 99/100, Train Loss: 6.2435, Test Loss: 2.0808\n",
      "Epoch 100/100, Train Loss: 6.2432, Test Loss: 2.0808\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Predictions: tensor([[ 4.9376e-05,  3.6706e+00,  1.8145e+00,  1.6405e+00, -1.7621e-02,\n",
      "          6.9007e-05],\n",
      "        [ 4.0880e-05,  3.6972e+00,  1.8996e+00,  1.5710e+00,  1.3074e-02,\n",
      "          7.2267e-05],\n",
      "        [ 1.9816e-05,  3.5972e+00,  1.7449e+00,  2.9297e+00,  6.6844e-02,\n",
      "          4.9152e-05],\n",
      "        ...,\n",
      "        [ 6.5140e-05,  3.2575e+00,  1.3406e+00,  4.3038e+00,  7.3836e-02,\n",
      "          8.0138e-05],\n",
      "        [ 6.5140e-05,  3.2575e+00,  1.3406e+00,  4.3038e+00,  7.3836e-02,\n",
      "          8.0138e-05],\n",
      "        [ 6.5140e-05,  3.2575e+00,  1.3406e+00,  4.3038e+00,  7.3836e-02,\n",
      "          8.0138e-05]], device='cuda:0')\n",
      "Actual Values: tensor([[1.0000, 3.5463, 1.7867, 1.6108, 0.0037, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0')\n",
      "Test Predictions: tensor([[1.9049e-05, 3.6085e+00, 1.7594e+00, 2.8780e+00, 6.5231e-02, 4.8279e-05],\n",
      "        [1.4057e-05, 3.6846e+00, 1.8564e+00, 2.5596e+00, 4.1360e-02, 3.9170e-05],\n",
      "        [1.7523e-05, 3.6109e+00, 1.7818e+00, 2.8112e+00, 2.4000e-02, 3.9738e-05],\n",
      "        ...,\n",
      "        [6.5166e-05, 3.2573e+00, 1.3406e+00, 4.3036e+00, 7.3838e-02, 8.0172e-05],\n",
      "        [6.5166e-05, 3.2573e+00, 1.3406e+00, 4.3036e+00, 7.3838e-02, 8.0172e-05],\n",
      "        [6.5166e-05, 3.2573e+00, 1.3406e+00, 4.3036e+00, 7.3838e-02, 8.0172e-05]],\n",
      "       device='cuda:0')\n",
      "Actual Values: tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Test Predictions: tensor([[1.9007e-05, 3.6092e+00, 1.7597e+00, 2.8790e+00, 6.5227e-02, 4.8168e-05],\n",
      "        [1.4026e-05, 3.6853e+00, 1.8567e+00, 2.5606e+00, 4.1356e-02, 3.9079e-05],\n",
      "        [1.7484e-05, 3.6116e+00, 1.7822e+00, 2.8122e+00, 2.3996e-02, 3.9647e-05],\n",
      "        ...,\n",
      "        [6.4740e-05, 3.2595e+00, 1.3416e+00, 4.3066e+00, 7.3831e-02, 7.9621e-05],\n",
      "        [6.4740e-05, 3.2595e+00, 1.3416e+00, 4.3066e+00, 7.3831e-02, 7.9621e-05],\n",
      "        [6.4740e-05, 3.2595e+00, 1.3416e+00, 4.3066e+00, 7.3831e-02, 7.9621e-05]],\n",
      "       device='cuda:0')\n",
      "Actual Values: tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 8\n",
    "hidden_dim = 128\n",
    "output_dim = 6\n",
    "num_gcn_layers = 1 \n",
    "num_rnn_layers = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "lambda_reg = 0.5\n",
    "lambda_class = 0.5\n",
    "\n",
    "# Model and optimizer\n",
    "model = GraphSequenceNN(input_dim, hidden_dim, output_dim, num_gcn_layers, num_rnn_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize empty lists to store loss values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "\n",
    "def evaluate_model(model, test_input_folder, batch_size, max_nodes, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    test_batch_generator, _ = load_sequence_data(test_input_folder, batch_size)\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_batch_generator:\n",
    "            test_data = prepare_batch(test_batch, max_nodes)\n",
    "            x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths = [t.to(device) for t in test_data]\n",
    "            output = model((x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths))\n",
    "            loss, _, _ = improved_loss_function(output, y, lambda_reg=lambda_reg, lambda_class=lambda_class)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "# Training and evaluation loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    batch_generator, max_nodes = load_sequence_data(train_input_folder, batch_size)\n",
    "    for batch in batch_generator:\n",
    "        data = prepare_batch(batch, max_nodes)\n",
    "        x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths = [t.to(device) for t in data]\n",
    "        optimizer.zero_grad()\n",
    "        output = model((x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths))\n",
    "        loss, reg_loss, class_loss = improved_loss_function(output, y, lambda_reg=lambda_reg, lambda_class=lambda_class)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    # Evaluation on test data\n",
    "    total_test_loss = evaluate_model(model, test_input_folder, batch_size, max_nodes, device)\n",
    "    \n",
    "    # Append the losses of this epoch to the lists\n",
    "    train_loss_values.append(total_train_loss)\n",
    "    test_loss_values.append(total_test_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_train_loss:.4f}, Test Loss: {total_test_loss:.4f}\")\n",
    "\n",
    "# Plot the loss values\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_loss_values, 'r', label='Training loss')\n",
    "plt.plot(test_loss_values, 'b', label='Test loss')\n",
    "plt.legend()\n",
    "plt.title('Loss values')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# Final evaluation on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch_generator, _ = load_sequence_data(test_input_folder, batch_size)\n",
    "    for test_batch in test_batch_generator:\n",
    "        test_data = prepare_batch(test_batch, max_nodes)\n",
    "        x_seq, edge_index_seq, x_last, edge_index_last, y, batch_last, seq_lengths = [t.to(device) for t in test_data]\n",
    "        predictions = model((x_seq, edge_index_seq, x_last, edge_index_last, batch_last, seq_lengths))\n",
    "        # Process predictions as needed\n",
    "        print(\"Test Predictions:\", predictions)\n",
    "        print(\"Actual Values:\", y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
